
@inproceedings{muller_efficient_2006,
	address = {Victoria},
	title = {An {Efficient} {Multiscale} {Approach} to {Audio} {Synchronization}},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Müller, Meinard and Mattes, Henning and Kurth, Frank},
	year = {2006},
	file = {Müller et al_2006_An Efficient Multiscale Approach to Audio Synchronization.pdf:H\:\\Docs\\zotero\\storage\\UNNBZC3M\\Müller et al_2006_An Efficient Multiscale Approach to Audio Synchronization.pdf:application/pdf}
}

@inproceedings{goto_music_1995,
	title = {Music {Understanding} {At} {The} {Beat} {Level} – {Real}-time {Beat} {Tracking} {For} {Audio} {Signals}},
	booktitle = {Proceedings of the {Workshop} on {Computational} {Auditory} {Scene} {Analysis} ({IJCAI})},
	author = {Goto, Masataka and Muraoka, Yoichi},
	month = aug,
	year = {1995},
	file = {Goto_Muraoka_1995_Music Understanding At The Beat Level – Real-time Beat Tracking For Audio.pdf:H\:\\Docs\\zotero\\storage\\2J4IV4TA\\Goto_Muraoka_1995_Music Understanding At The Beat Level – Real-time Beat Tracking For Audio.pdf:application/pdf}
}

@techreport{16:1975_acoustics_1975,
	type = {Standard},
	title = {Acoustics â€“ {Standard} tuning frequency ({Standard} musical pitch)},
	institution = {ISO},
	author = {16:1975, ISO},
	year = {1975}
}

@article{mcadams_hearing_1979,
	title = {Hearing {Musical} {Streams}},
	volume = {3},
	copyright = {Copyright {\textcopyright} 1979 The MIT Press},
	issn = {0148-9267},
	number = {4},
	journal = {Computer Music Journal},
	author = {McAdams, Stephen and Bregman, Albert},
	month = dec,
	year = {1979},
	pages = {26--60},
	file = {McAdams_Bregman_1979_Hearing Musical Streams.pdf:H\:\\Docs\\zotero\\storage\\PRUDRQWA\\McAdams_Bregman_1979_Hearing Musical Streams.pdf:application/pdf}
}

@book{bregman_auditory_1994,
	title = {Auditory {Scene} {Analysis}},
	publisher = {MIT Press},
	author = {Bregman, Albert S},
	year = {1994},
	annote = {1st Paperback Edition}
}

@article{hammershoi_methods_2002,
	title = {Methods for {Binaural} {Recording} and {Reproduction}},
	volume = {88},
	abstract = {The term binaural technique is used as a headline for methods for sound recording, synthesis and reproduction, where the sound pressure signals recorded (or synthesized) are the eardrum signals of the listener, and where successful reproduction is achieved, if these are truely reproduced in the ears of the listener. This paper shortly summarizes the main results of a series of investigations that are the current basis for the utilization of the binaural technique at Aalborg University and possibly elsewhere, including investigations on sound transmission in the ear canal, measurements of HRTFs (head-related transfer functions), calibration of headphones and localization experiments in real life and with binaural recordings from real heads and artificial heads.},
	number = {3},
	journal = {Acta Acustica united with Acustica},
	author = {Hammersh{\o}i, Dorte and M{\o}ller, Henrik},
	month = may,
	year = {2002},
	pages = {303--311},
	file = {Hammersh{\o}i_M{\o}ller_2002_Methods for Binaural Recording and Reproduction.pdf:H\:\\Docs\\zotero\\storage\\M8ENMMUZ\\Hammersh{\o}i_M{\o}ller_2002_Methods for Binaural Recording and Reproduction.pdf:application/pdf}
}

@misc{schleske_website,
    title = {Vibrato of the Musician},
    author = {Martin Schleske},
    url = {http://www.schleske.de/en/our-research/handbook-violinacoustics/vibrato-of-the-musician.html},
    urldate = {2015-07-29}
}

@misc{specloudness_website,
    title = {Customised metrics},
    author = {University of Salford},
    url = {https://www.salford.ac.uk/computing-science-engineering/research/acoustics/psychoacoustics/sound-quality-making-products-sound-better/sound-quality-testing/customised-metrics},
    urldate = {2015-07-29}
}

@techreport{asa_acoustical_1960,
	type = {Standard},
	title = {Acoustical {Terminology}},
	institution = {American Standards Association (ASA)},
	author = {ASA},
	year = {1960}
}

@inproceedings{noll_pitch_1969,
	address = {Brooklyn},
	title = {Pitch {Determination} of {Human} {Speech} by the {Harmonic} {Product} {Spectrum}, the {Harmonic} {Sum} {Spectrum}, and a {Maximum} {Likelihood} {Estimate}},
	volume = {19},
	booktitle = {Proceedings of the {Symposium} on {Computer} {Processing} in {Communications}},
	publisher = {Polytechnic Press of the University of Brooklyn},
	author = {Noll, A Michael},
	year = {1969},
	pages = {779--797},
	file = {Noll_1969_Pitch Determination of Human Speech by the Harmonic Product Spectrum, the.pdf:H\:\\Docs\\zotero\\storage\\EWSRH7KE\\Noll_1969_Pitch Determination of Human Speech by the Harmonic Product Spectrum, the.pdf:application/pdf}
}

@misc{cuadra_website,
    title = {Pitch detection methods review},
    author = {Patricio de la Cuadra },
    url = {https://ccrma.stanford.edu/~pdelac/154/m154paper.htm},
    urldate = {2015-08-04}
}

@inproceedings{klapuri_perceptually_2005,
	address = {New Paltz},
	title = {A {Perceptually} {Motivated} {Multiple}-{F}0 {Estimation} {Method}},
	booktitle = {Proceedings of the {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	author = {Klapuri, Anssi P},
	year = {2005},
	file = {Klapuri_2005_A Perceptually Motivated Multiple-F0 Estimation Method.pdf:H\:\\Docs\\zotero\\storage\\AC69ZBPC\\Klapuri_2005_A Perceptually Motivated Multiple-F0 Estimation Method.pdf:application/pdf}
}

@inproceedings{lerch_ansatz_2004,
	address = {Leipzig},
	title = {Ein {Ansatz} zur automatischen {Erkennung} der {Tonart} in {Musikdateien}},
	abstract = {Es wird ein Verfahren zur automatischen Erkennung der Tonart von Musikdateien vorgestellt. Das Verfahren analysiert mittels einer Filterbank den Tonvorrat des Eingangssignals, der in einem Tonvektor zusammenfasst wird. Dabei sind sowohl mehrstimmige als auch ein- stimmige Eingangssignale zul{\"a}ssig. Mit Hilfe eines Nearest-Neighbour-Classifiers wird anschlie{\ss} end das wahrscheinlichste Ergebnis f{\"u}r den extrahierten Tonvektor bestimmt. Parallel zur Analyse des Tonvorrats wird die Stimmh{\"o}he des Kammertons detektiert, um eine gleichbleibende Erkennungsrate f{\"u}r Signale unterschiedlicher Stimmh{\"o}he zu gew{\"a}hrleisten.},
	booktitle = {Proceedings of the {VDT} {International} {Audio} {Convention} (23. {Tonmeistertagung})},
	author = {Lerch, Alexander},
	month = nov,
	year = {2004},
	file = {Lerch_2004_Ein Ansatz zur automatischen Erkennung der Tonart in Musikdateien.pdf:H\:\\Docs\\zotero\\storage\\WAM57AHA\\Lerch_2004_Ein Ansatz zur automatischen Erkennung der Tonart in Musikdateien.pdf:application/pdf}
}

@inproceedings{lerch_requirement_2006,
	address = {Victoria},
	title = {On the {Requirement} of {Automatic} {Tuning} {Frequency} {Estimation}},
	abstract = {The deviation of the tuning frequency from the standard tuning frequency 440 Hz is evaluated for a database of classical music. It is discussed if and under what circumstances such a deviation may affect the robustness of pitch-based systems for musical content analysis.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Lerch, Alexander},
	year = {2006},
	keywords = {frequency, tuning},
}


@article{hirsh_auditory_1959,
	title = {Auditory {Perception} of {Temporal} {Order}},
	volume = {31},
	abstract = {Whereas temporal intervals as short as a few milliseconds are sufficient to separate two brief sounds so that a listener will report that there are two (instead of only one) sounds, a longer separation time of between 15 and 20 msec is required for the listener to report correctly which of the two sounds preceded the other. This minimum temporal interval appears to be independent of the kinds of sounds used: whether short or long, of high or low frequency, of narrow or wide band width. There is some suggestion that rise-time and duration may change this minimum interval, but these somewhat secondary relations are not investigated in detail here. The length of the required temporal interval suggests that the judgment of order requires other mechanisms than those associated with the peripheral auditory system.},
	number = {6},
	journal = {JASA},
	author = {Hirsh, Ira J},
	year = {1959},
	pages = {759},
	file = {Hirsh_1959_Auditory Perception of Temporal Order.pdf:H\:\\Docs\\zotero\\storage\\44RA424J\\Hirsh_1959_Auditory Perception of Temporal Order.pdf:application/pdf}
}

@article{rasch_synchronization_1979,
	title = {Synchronization in {Performed} {Ensemble} {Music}},
	volume = {43},
	journal = {Acustica},
	author = {Rasch, Rudolf A},
	year = {1979},
	pages = {121--131}
}

@phdthesis{gordon_perception_1984,
	address = {Stanford},
	type = {Dissertation},
	title = {Perception of {Attack} {Transients} in {Musical} {Tones}},
	school = {Stanford University, Center for Computer Research in Music and Acoustics (CCRMA)},
	author = {Gordon, John William},
	year = {1984},
	file = {Gordon_1984_Perception of Attack Transients in Musical Tones.pdf:H\:\\Docs\\zotero\\storage\\N3K5SBHH\\Gordon_1984_Perception of Attack Transients in Musical Tones.pdf:application/pdf}
}

@article{shaffer_timing_1984,
	title = {Timing in {Solo} and {Duet} {Piano} {Performances}},
	volume = {36A},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {Shaffer, L Henry},
	year = {1984},
	pages = {577--595},
	file = {Shaffer_1984_Timing in Solo and Duet Piano Performances.pdf:H\:\\Docs\\zotero\\storage\\F9XZC32E\\Shaffer_1984_Timing in Solo and Duet Piano Performances.pdf:application/pdf}
}

@article{friberg_perception_1992,
	title = {Perception of just noticeable time displacement of a tone presented in a {Metrical} {Sequence} at {Different} {Tempos}},
	volume = {33},
	number = {4},
	journal = {STL-QPSR},
	author = {Friberg, Anders and Sundberg, Johan},
	year = {1992},
	pages = {97--108},
	file = {Friberg_Sundberg_1992_Perception of just noticeable time displacement of a tone presented in a.pdf:H\:\\Docs\\zotero\\storage\\UPJR2RQI\\Friberg_Sundberg_1992_Perception of just noticeable time displacement of a tone presented in a.pdf:application/pdf}
}

@article{repp_diversity_1992,
	title = {Diversity and commonality in music performance: {An} analysis of timing microstructure in {Schumann}'s '{Tr{\"a}umerei}'},
	volume = {92},
	number = {5},
	journal = {JASA},
	author = {Repp, Bruno H},
	year = {1992},
	pages = {2546--2568},
	file = {Repp_1992_Diversity and commonality in music performance.pdf:H\:\\Docs\\zotero\\storage\\K795W53K\\Repp_1992_Diversity and commonality in music performance.pdf:application/pdf}
}

@inproceedings{leveau_methodology_2004,
	address = {Barcelona},
	title = {Methodology and {Tools} for the {Evaluation} of {Automatic} {Onset} {Detection} {Algorithms} in {Music}},
	booktitle = {{ISMIR}},
	author = {Leveau, Pierre and Daudet, Laurent and Richard, Ga{\"e}l},
	year = {2004},
	file = {Leveau et al_2004_Methodology and Tools for the Evaluation of Automatic Onset Detection.pdf:H\:\\Docs\\zotero\\storage\\WMWNEPDB\\Leveau et al_2004_Methodology and Tools for the Evaluation of Automatic Onset Detection.pdf:application/pdf}
}

@inproceedings{collins_using_2005,
	title = {Using a pitch detector for onset detection},
	booktitle = {{ISMIR}},
	author = {Collins, Nick},
	year = {2005},
	keywords = {onset detection, pitch detection, segmenta-},
	pages = {100--106},
	file = {Collins_2005_Using a pitch detector for onset detection.pdf:H\:\\Docs\\zotero\\storage\\T6AHX6CA\\Collins_2005_Using a pitch detector for onset detection.pdf:application/pdf;Collins_2005_Using a pitch detector for onset detection.pdf:H\:\\Docs\\zotero\\storage\\ZTPM5PEK\\Collins_2005_Using a pitch detector for onset detection.pdf:application/pdf}
}

@inproceedings{large_beat_1995,
	address = {Montreal},
	title = {Beat {Tracking} with a {Nonlinear} {Oscillator}},
	booktitle = {Proceedings of the 14th {International} {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI})},
	author = {Large, Edward Wilson},
	month = aug,
	year = {1995},
	file = {Large_1995_Beat Tracking with a Nonlinear Oscillator.pdf:H\:\\Docs\\zotero\\storage\\CIQZSSR7\\Large_1995_Beat Tracking with a Nonlinear Oscillator.pdf:application/pdf}
}

@article{scheirer_tempo_1998,
	title = {Tempo and beat analysis of acoustic musical signals},
	volume = {103},
	number = {1},
	journal = {Journal of the Acoustical Society of America (JASA)},
	author = {Scheirer, Eric D},
	year = {1998},
	pages = {588--601},
	file = {Scheirer_1998_Tempo and beat analysis of acoustic musical signals.pdf:H\:\\Docs\\zotero\\storage\\Z9IPJV2D\\Scheirer_1998_Tempo and beat analysis of acoustic musical signals.pdf:application/pdf}
}

@article{burred_hierarchical_2004,
	title = {Hierarchical {Automatic} {Audio} {Signal} {Classification}},
	volume = {52},
	abstract = {The design, implementation, and evaluation of a system for automatic audio signal classification is presented. The signals are classified according to audio type, differentiating between three speech classes, 13 musical genres, and background noise. A large number of audio features are evaluated for their suitability in such a classification task, including MPEG-7 descriptors and several new features. The selection of the features is carried out systematically with regard to their robustness to noise and bandwidth changes, as well as to their ability to distinguish a given set of audio types. Direct and hierarchical approaches for the feature selection and for the classification are evaluated and compared.},
	number = {7/8},
	journal = {Journal of the Audio Engineering Society (JAES)},
	author = {Burred, Juan Jos{\'e} and Lerch, Alexander},
	year = {2004},
	pages = {724--739},
	file = {Burred_Lerch_2004_Hierarchical Automatic Audio Signal Classification.pdf:H\:\\Docs\\zotero\\storage\\WR34F49E\\Burred_Lerch_2004_Hierarchical Automatic Audio Signal Classification.pdf:application/pdf}
}

@inproceedings{seung_nmf_2001,
author = {Seung, D and Lee, L},
booktitle = {Advances in neural information processing systems},
file = {:Users/mac/Documents/Mendeley Desktop/Seung, Lee/Advances in neural information processing systems/Seung, Lee - 2001 - Algorithms for non-negative matrix factorization.pdf:pdf},
mendeley-groups = {Classification,NMF related,CW ISMIR 2014},
number = {13},
pages = {556--562},
title = {{Algorithms for non-negative matrix factorization}},
url = {http://www.public.asu.edu/~jye02/CLASSES/Fall-2007/NOTES/lee01algorithms.pdf},
year = {2001}
}

@article{smaragdis_nmf_2003,
author = {Smaragdis, P and Brown, JC},
file = {:Users/mac/Documents/Mendeley Desktop/Smaragdis, Brown/Applications of Signal Processing to Audio and Acoustics, 2003 IEEE Workshop on/Smaragdis, Brown - 2003 - Non-negative matrix factorization for polyphonic music transcription.pdf:pdf},
isbn = {0780378504},
journal = {Applications of Signal Processing to Audio and Acoustics, 2003 IEEE Workshop on.},
mendeley-groups = {Transcription,NMF related,CW ISMIR 2014},
pages = {177--180},
title = {{Non-negative matrix factorization for polyphonic music transcription}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=869637 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1285860},
year = {2003}
}

@book{cichocki_nmf_2009,
author = {Cichocki, A and Zdunek, R and Phan, AH and Amari, S},
mendeley-groups = {NMF related},
publisher = {John Wiley \& Sons},
title = {{Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation}},
year = {2009}
}


@inproceedings{lykartsis_rhythm_2015,
	address = {Trondheim, Norway},
	title = {Rhythm {Features} for {Musical} {Genre} {Classification} {Using} {Multiple} {Novelty} {Functions}},
	booktitle = {Proceedings of the {International} {Conference} on {Digital} {Audio} {Effects} ({DAFX})},
	author = {Lykartsis, Athanasios and Lerch, Alexander},
	year = {2015},
	file = {Lykartsis_Lerch_2015_Rhythm Features for Musical Genre Classification Using Multiple Novelty.pdf:H\:\\Docs\\zotero\\storage\\WZP9NVDE\\DAFx-15_submission_42 (1).pdf:application/pdf}
}


@inproceedings{dixon_match:_2005,
	address = {London},
	title = {{MATCH}: {A} {Music} {Alignment} {Tool} {Chest}},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Dixon, Simon and Widmer, Gerhard},
	month = sep,
	year = {2005},
	file = {Dixon_Widmer_2005_MATCH.pdf:H\:\\Docs\\zotero\\storage\\8WAPZWZD\\Dixon_Widmer_2005_MATCH.pdf:application/pdf}
}


@article{kirchhoff_evaluation_2011,
	title = {Evaluation of {Features} for {Audio}-to-{Audio} {Alignment}},
	volume = {40},
	doi = {10.1080/09298215.2010.529917},
	number = {1},
	journal = {Journal of New Music Research},
	author = {Kirchhoff, Holger and Lerch, Alexander},
	year = {2011},
	pages = {27--41},
	file = {Kirchhoff_Lerch_2011_Evaluation of Features for Audio-to-Audio Alignment.pdf:H\:\\Docs\\zotero\\storage\\JX2SU8Z7\\Kirchhoff_Lerch_2011_Evaluation of Features for Audio-to-Audio Alignment.pdf:application/pdf}
}

@article{gjerdingen_scanning_2008,
	title = {Scanning the {Dial}: {The} {Rapid} {Recognition} of {Music} {Genres}},
	volume = {37},
	issn = {0929-8215},
	number = {2},
	journal = {Journal of New Music Research},
	author = {Gjerdingen, Robert O and Perrott, David},
	month = jun,
	year = {2008},
	note = {00067},
	pages = {93--100},
	file = {Gjerdingen_Perrott_2008_Scanning the Dial.pdf:H\:\\Docs\\zotero\\storage\\SQ2U2AF7\\Gjerdingen_Perrott_2008_Scanning the Dial.pdf:application/pdf}
}

@inproceedings{lippens_comparison_2004,
	address = {Montreal},
	title = {A {Comparison} of {Human} and {Automatic} {Musical} {Genre} {Classification}},
	booktitle = {Proceedings of the {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing} ({ICASSP})},
	author = {Lippens, Stefaan and Martens, Jean-Pierre and Mulder, Tom De and Tzanetakis, George},
	year = {2004},
	file = {Lippens et al_2004_A Comparison of Human and Automatic Musical Genre Classification.pdf:H\:\\Docs\\zotero\\storage\\JSCKPWRZ\\Lippens et al_2004_A Comparison of Human and Automatic Musical Genre Classification.pdf:application/pdf}
}

@article{tzanetakis_musical_2002,
	title = {Musical genre classification of audio signals},
	volume = {10},
	issn = {1063-6676},
	doi = {10.1109/TSA.2002.800560},
	number = {5},
	journal = {Transactions on Speech and Audio Processing},
	author = {Tzanetakis, George and Cook, Perry},
	month = jul,
	year = {2002},
	pages = {293--302},
	file = {Tzanetakis_Cook_2002_Musical genre classification of audio signals.pdf:H\:\\Docs\\zotero\\storage\\B95EI5NS\\Tzanetakis_Cook_2002_Musical genre classification of audio signals.pdf:application/pdf}
}

@inproceedings{tzanetakis_pitch_2002,
	address = {Paris},
	title = {Pitch {Histograms} in {Audio} and {Symbolic} {Music} {Information} {Retrieval}},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Tzanetakis, George and Ermolinskyi, Andrey and Cook, Perry},
	year = {2002},
	file = {Tzanetakis et al_2002_Pitch Histograms in Audio and Symbolic Music Information Retrieval.pdf:H\:\\Docs\\zotero\\storage\\GT2WNCVK\\Tzanetakis et al_2002_Pitch Histograms in Audio and Symbolic Music Information Retrieval.pdf:application/pdf}
}

@phdthesis{pampalk_islands_2001,
	type = {Diploma {Thesis}},
	title = {Islands of {Music}},
	school = {Technische Universit\"at Wien},
	author = {Pampalk, Elias},
	year = {2001},
}

@article{russel_circumplex_1980,
	title = {A {Circumplex} {Model} of {Affect}},
	volume = {39},
	issn = {1939-1315(Electronic);0022-3514(Print)},
	doi = {10.1037/h0077714},
	abstract = {Factor-analytic evidence has led most psychologists to describe affect as a set of dimensions, such as displeasure, distress, depression, excitement, and so on, with each dimension varying independently of the others. However, there is other evidence that rather than being independent, these affective dimensions are interrelated in a highly systematic fashion. The evidence suggests that these interrelationships can be represented by a spatial model in which affective concepts fall in a circle in the following order: pleasure (0), excitement (45), arousal (90), distress (135), displeasure (180), depression (225), sleepiness (270), and relaxation (315). This model was offered both as a way psychologists can represent the structure of affective experience, as assessed through self-report, and as a representation of the cognitive structure that laymen utilize in conceptualizing affect. Supportive evidence was obtained by scaling 28 emotion-denoting adjectives in 4 different ways: R. T. Ross's (1938) technique for a circular ordering of variables, a multidimensional scaling procedure based on perceived similarity among the terms, a unidimensional scaling on hypothesized pleasure–displeasure and degree-of-arousal dimensions, and a principal-components analysis of 343 Ss' self-reports of their current affective states. (70 ref)},
	number = {6},
	journal = {Journal of Personality and Social Psychology},
	author = {Russel, James A},
	year = {1980},
	keywords = {*Emotional States, *Emotions, Models},
	pages = {1161--1178},
}

@inproceedings{haitsma_highly_2002,
	address = {Paris},
	title = {A {Highly} {Robust} {Audio} {Fingerprinting} {System}},
	booktitle = {Proceedings of the {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Haitsma, Jaap and Kalker, Ton},
	year = {2002},
	file = {Haitsma_Kalker_2002_A Highly Robust Audio Fingerprinting System.pdf:H\:\\Docs\\zotero\\storage\\GWHZKUPI\\Haitsma_Kalker_2002_A Highly Robust Audio Fingerprinting System.pdf:application/pdf}
}

@inproceedings{wang_industrial_2003,
	address = {Washington},
	title = {An {Industrial} {Strength} {Audio} {Search} {Algorithm}},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Wang, A.},
	year = {2003},
	file = {Wang_2003_An Industrial Strength Audio Search Algorithm.pdf:H\:\\Docs\\zotero\\storage\\74SI9W8F\\Wang_2003_An Industrial Strength Audio Search Algorithm.pdf:application/pdf}
}

@inproceedings{paulus_audio-based_2010,
	address = {Utrecht},
	title = {Audio-{Based} {Music} {Structure} {Analysis}},
	abstract = {Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-the-art methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneity-based approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field.},
	booktitle = {Proceedings of the {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Paulus, Jouni K and M\"uller, Meinard and Klapuri, Anssi P},
	year = {2010},
	file = {ismir2010-107.pdf:H\:\\Docs\\zotero\\storage\\AD3FGBJK\\ismir2010-107.pdf:application/pdf}
}